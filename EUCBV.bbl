\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }

\bibitem{agrawal2011analysis}
Agrawal, S., Goyal, N.: Analysis of thompson sampling for the multi-armed
  bandit problem. arXiv preprint arXiv:1111.1797  (2011)

\bibitem{audibert2009minimax}
Audibert, J.Y., Bubeck, S.: Minimax policies for adversarial and stochastic
  bandits. In: COLT. pp. 217--226 (2009)

\bibitem{audibert2010best}
Audibert, J.Y., Bubeck, S.: Best arm identification in multi-armed bandits. In:
  COLT-23th COLT-2010. pp. 13--p (2010)

\bibitem{audibert2009exploration}
Audibert, J.Y., Munos, R., Szepesv{\'a}ri, C.: Exploration--exploitation
  tradeoff using variance estimates in multi-armed bandits. TCS  410(19),
  1876--1902 (2009)

\bibitem{auer2002finite}
Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite-time analysis of the multiarmed
  bandit problem. Machine learning  47(2-3),  235--256 (2002)

\bibitem{auer2002nonstochastic}
Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R.E.: The nonstochastic
  multiarmed bandit problem. SICOMP  32(1),  48--77 (2002)

\bibitem{auer2010ucb}
Auer, P., Ortner, R.: Ucb revisited: Improved regret bounds for the stochastic
  multi-armed bandit problem. Periodica Mathematica Hungarica  61(1-2),  55--65
  (2010)

\bibitem{bubeck2012regret}
Bubeck, S., Cesa-Bianchi, N.: Regret analysis of stochastic and nonstochastic
  multi-armed bandit problems. arXiv preprint arXiv:1204.5721  (2012)

\bibitem{bubeck2012bandits}
Bubeck, S., Cesa-Bianchi, N., Lugosi, G.: Bandits with heavy tail. arXiv
  preprint arXiv:1209.1727  (2012)

\bibitem{bubeck2011pure}
Bubeck, S., Munos, R., Stoltz, G.: Pure exploration in finitely-armed and
  continuous-armed bandits. TCS  412(19),  1832--1852 (2011)

\bibitem{CapGarKau12}
Cappe, O., Garivier, A., Kaufmann, E.: pymabandits (2012),
  \url{http://mloss.org/software/view/415/}

\bibitem{even2006action}
Even-Dar, E., Mannor, S., Mansour, Y.: Action elimination and stopping
  conditions for the multi-armed bandit and reinforcement learning problems.
  JMLR  7,  1079--1105 (2006)

\bibitem{garivier2011kl}
Garivier, A., Capp{\'e}, O.: The kl-ucb algorithm for bounded stochastic
  bandits and beyond. arXiv preprint arXiv:1102.2490  (2011)

\bibitem{honda2010asymptotically}
Honda, J., Takemura, A.: An asymptotically optimal bandit algorithm for bounded
  support models. In: COLT. pp. 67--79. Citeseer (2010)

\bibitem{kaufmann2012bayesian}
Kaufmann, E., Capp{\'e}, O., Garivier, A.: On bayesian upper confidence bounds
  for bandit problems. In: AISTATS. pp. 592--600 (2012)

\bibitem{lai1985asymptotically}
Lai, T.L., Robbins, H.: Asymptotically efficient adaptive allocation rules.
  Advances in applied mathematics  6(1),  4--22 (1985)

\bibitem{lattimore2015optimally}
Lattimore, T.: Optimally confident ucb: Improved regret for finite-armed
  bandits. arXiv preprint arXiv:1507.07880  (2015)

\bibitem{liu2016modification}
Liu, Y.C., Tsuruoka, Y.: Modification of improved upper confidence bounds for
  regulating exploration in monte-carlo tree search. TCS  (2016)

\bibitem{robbins1952some}
Robbins, H.: Some aspects of the sequential design of experiments. In: Herbert
  Robbins Selected Papers, pp. 169--177. Springer (1952)

\bibitem{thompson1933likelihood}
Thompson, W.R.: On the likelihood that one unknown probability exceeds another
  in view of the evidence of two samples. Biometrika pp. 285--294 (1933)

\end{thebibliography}
