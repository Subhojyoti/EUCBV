\begin{thebibliography}{}

\bibitem[Agrawal and Goyal, 2011]{agrawal2011analysis}
Agrawal, S. and Goyal, N. (2011).
\newblock Analysis of thompson sampling for the multi-armed bandit problem.
\newblock {\em arXiv preprint arXiv:1111.1797}.

\bibitem[Audibert and Bubeck, 2009]{audibert2009minimax}
Audibert, J.-Y. and Bubeck, S. (2009).
\newblock Minimax policies for adversarial and stochastic bandits.
\newblock In {\em COLT}, pages 217--226.

\bibitem[Audibert and Bubeck, 2010]{audibert2010best}
Audibert, J.-Y. and Bubeck, S. (2010).
\newblock Best arm identification in multi-armed bandits.
\newblock In {\em COLT-23th Conference on Learning Theory-2010}, pages 13--p.

\bibitem[Audibert et~al., 2009]{audibert2009exploration}
Audibert, J.-Y., Munos, R., and Szepesv{\'a}ri, C. (2009).
\newblock Exploration--exploitation tradeoff using variance estimates in
  multi-armed bandits.
\newblock {\em Theoretical Computer Science}, 410(19):1876--1902.

\bibitem[Auer et~al., 2002a]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002a).
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine learning}, 47(2-3):235--256.

\bibitem[Auer et~al., 2002b]{auer2002nonstochastic}
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.~E. (2002b).
\newblock The nonstochastic multiarmed bandit problem.
\newblock {\em SIAM Journal on Computing}, 32(1):48--77.

\bibitem[Auer and Ortner, 2010]{auer2010ucb}
Auer, P. and Ortner, R. (2010).
\newblock Ucb revisited: Improved regret bounds for the stochastic multi-armed
  bandit problem.
\newblock {\em Periodica Mathematica Hungarica}, 61(1-2):55--65.

\bibitem[Bubeck and Cesa-Bianchi, 2012]{bubeck2012regret}
Bubeck, S. and Cesa-Bianchi, N. (2012).
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock {\em arXiv preprint arXiv:1204.5721}.

\bibitem[Bubeck et~al., 2012]{bubeck2012bandits}
Bubeck, S., Cesa-Bianchi, N., and Lugosi, G. (2012).
\newblock Bandits with heavy tail.
\newblock {\em arXiv preprint arXiv:1209.1727}.

\bibitem[Bubeck et~al., 2011]{bubeck2011pure}
Bubeck, S., Munos, R., and Stoltz, G. (2011).
\newblock Pure exploration in finitely-armed and continuous-armed bandits.
\newblock {\em Theoretical Computer Science}, 412(19):1832--1852.

\bibitem[Cappe et~al., 2012]{CapGarKau12}
Cappe, O., Garivier, A., and Kaufmann, E. (2012).
\newblock pymabandits.
\newblock \url{http://mloss.org/software/view/415/}.

\bibitem[Even-Dar et~al., 2006]{even2006action}
Even-Dar, E., Mannor, S., and Mansour, Y. (2006).
\newblock Action elimination and stopping conditions for the multi-armed bandit
  and reinforcement learning problems.
\newblock {\em The Journal of Machine Learning Research}, 7:1079--1105.

\bibitem[Garivier and Capp{\'e}, 2011]{garivier2011kl}
Garivier, A. and Capp{\'e}, O. (2011).
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock {\em arXiv preprint arXiv:1102.2490}.

\bibitem[Honda and Takemura, 2010]{honda2010asymptotically}
Honda, J. and Takemura, A. (2010).
\newblock An asymptotically optimal bandit algorithm for bounded support
  models.
\newblock In {\em COLT}, pages 67--79. Citeseer.

\bibitem[Kaufmann et~al., 2012]{kaufmann2012bayesian}
Kaufmann, E., Capp{\'e}, O., and Garivier, A. (2012).
\newblock On bayesian upper confidence bounds for bandit problems.
\newblock In {\em AISTATS}, pages 592--600.

\bibitem[Lattimore, 2015]{lattimore2015optimally}
Lattimore, T. (2015).
\newblock Optimally confident ucb: Improved regret for finite-armed bandits.
\newblock {\em arXiv preprint arXiv:1507.07880}.

\bibitem[Liu and Tsuruoka, 2016]{liu2016modification}
Liu, Y.-C. and Tsuruoka, Y. (2016).
\newblock Modification of improved upper confidence bounds for regulating
  exploration in monte-carlo tree search.
\newblock {\em Theoretical Computer Science}.

\end{thebibliography}
