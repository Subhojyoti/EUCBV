In this paper we deal with the stochastic multi-armed bandit (MAB) setting. Stochastic MABs are classic instances of sequential learning model where at each timestep a learner is exposed to a finite set of actions (or arms) and it has to choose one arm at a time. After choosing (or pulling) an arm the learner sees the reward for the arm as revealed by the environment. Each of these rewards is an i.i.d random variable as sampled from the distribution associated with each arm. The mean of the reward distribution associated with an arm $i$ is denoted by $r_i$ whereas the mean of the reward distribution of the optimal arm $*$ is denoted by $r^*$ such that $r_i < r^*, \forall i\in \A$. One of the fundamental assumptions in stochastic MAB is that the distribution associated with each arm does not change over the entire time horizon $T$. The objective in the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in \A} r_{i}n_{i}(T),
\end{align*}
where $T$ is the number of timesteps, $n_{i}(T)=\sum_{j=1}^T I(I_j=i)$ is the number of times the algorithm has chosen arm $i$ up to timestep $T$.
The expected regret of an algorithm after $T$ timesteps can be written as,
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[n_i (T)] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ is the gap between the means of the optimal arm and the $i$-th arm.