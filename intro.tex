In this paper we deal with the stochastic multi-armed bandit (MAB) setting. Stochastic MABs are classic instances of sequential learning model where at each timestep a learner is exposed to a finite set of actions (or arms) and it has to choose one arm at a time. After choosing (or pulling) an arm the learner sees the reward for the arm as revealed by the environment. Each of these rewards is an i.i.d random variable as sampled from the distribution associated with each arm. The mean of the reward distribution associated with an arm $i$ is denoted by $r_i$ whereas the mean of the reward distribution of the optimal arm $*$ is denoted by $r^*$ such that $r_i < r^*, \forall i\in \A$. So at every timestep the learner faces the task of balancing exploitation and exploration, that is whether to pull the arm for which it has seen the best estimates or to explore more arms in the hope of finding better performing arms. One of the fundamental assumptions in stochastic MAB is that the distribution associated with each arm does not change over the entire time horizon $T$. The objective in the stochastic bandit problem is to minimize the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in \A} r_{i}z_{i}(T),
\end{align*}
where $T$ is the number of timesteps, $z_{i}(T)=\sum_{j=1}^T I(I_j=i)$ is the number of times the algorithm has chosen arm $i$ up to timestep $T$.
The expected regret of an algorithm after $T$ timesteps can be written as,
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[z_i (T)] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ is the gap between the means of the optimal arm and the $i$-th arm.

	In recent years the MAB setting has garnered extensive popularity because of its simple learning  model and its practical applications in a wide-range of industry defined problems. From the area of mobile channel allocations to active learning or computer simulation games, MABs have become an increasingly useful tool which has been implemented successfully in a wide range of applications. 